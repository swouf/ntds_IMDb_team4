{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 3: spectral graph theory\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Michaël Defferrard](http://deff.ch), [EPFL LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `4`\n",
    "* Students: `Julien Berger, Jérémy Jayet, Hana Samet, Mathieu Shiva`\n",
    "* Dataset: `IMDb Films and Crew`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this milestone is to get familiar with the graph Laplacian and its spectral decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `No module named 'sklearn'` error when running the below cell, install [scikit-learn](https://scikit-learn.org) with `conda install scikit-learn` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote your graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, A)$, where $\\mathcal{V}$ is the set of nodes, $\\mathcal{E}$ is the set of edges, $A \\in \\mathbb{R}^{N \\times N}$ is the (weighted) adjacency matrix, and $N = |\\mathcal{V}|$ is the number of nodes.\n",
    "\n",
    "Import the adjacency matrix $A$ that you constructed in the first milestone.\n",
    "(You're allowed to update it between milestones if you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = np.load('./data/adjacency.npy')\n",
    "    \n",
    "#We calculate the number of nodes   \n",
    "n_nodes=adjacency.shape[0]\n",
    "\n",
    "#We make the matrix more sparse by removing the link between two people if they worked only on 1 movie together\n",
    "adjacency[adjacency <2]=0\n",
    "\n",
    "n_edges =  np.count_nonzero(adjacency)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "From the (weighted) adjacency matrix $A$, compute both the combinatorial (also called unnormalized) and the normalized graph Laplacian matrices.\n",
    "\n",
    "Note: if your graph is weighted, use the weighted adjacency matrix. If not, use the binary adjacency matrix.\n",
    "\n",
    "For efficient storage and computation, store these sparse matrices in a [compressed sparse row (CSR) format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore')\n",
    "adjacency_csr= sparse.csr_matrix(adjacency)\n",
    "\n",
    "degree_csr= sparse.spdiags(np.sum(adjacency, axis=1),0,n_nodes,n_nodes)\n",
    "\n",
    "laplacian_combinatorial =degree_csr-adjacency_csr\n",
    "\n",
    "laplacian_normalized = degree_csr.power(-1/2)*laplacian_combinatorial*degree_csr.power(-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of them as the graph Laplacian $L$ for the rest of the milestone.\n",
    "We however encourage you to run the code with both to get a sense of the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian =laplacian_normalized  # Either laplacian_combinatorial or laplacian_normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute the eigendecomposition of the Laplacian $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues.\n",
    "\n",
    "Make sure that the eigenvalues are ordered, i.e., $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors= np.linalg.eigh(laplacian.toarray())\n",
    "\n",
    "indx=np.argsort(eigenvalues)\n",
    "\n",
    "eigenvalues=eigenvalues[indx]\n",
    "eigenvectors=eigenvectors[:,indx]\n",
    "\n",
    "assert eigenvectors.shape == (n_nodes, n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify your choice of eigensolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried using *sparse.linalg.eigs* with the sparse matrix but it couldn't accept K bigger than n_nodes-2, so we couldn't compute all eigenvalues/vectors. Also *sparse.linalg.eigs* took a lot of time, more than 2 hours. *sparse.linalg.eigs* is a solver that is more appropriate for when you need only a limited subset of the eigenvalues/vectors.\n",
    "\n",
    "So, instead we used *np.linalg.eigh* which was a lot faster. Also, it takes the dense laplacian matrix and returns all of the eigenvalues/vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We can write $L = S S^\\top$. What is the matrix $S$? What does $S^\\top x$, with $x \\in \\mathbb{R}^N$, compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S is the incidence matrix :\n",
    "\n",
    "S( i,j )= +1 if $e_j =(v_i,v_k)$ for some k \n",
    "\n",
    "or S( i,j )= -1 if $e_j =(v_k,v_i)$ for some k\n",
    "\n",
    "or S( i,j )=0 otherwise\n",
    "\n",
    "with $x \\in \\mathbb{R}^N$  $S^\\top x$ compute the gradient of x \n",
    "\n",
    "$S^\\top x \\in \\mathbb{R}^m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Show that $\\lambda_k = \\| S^\\top u_k \\|_2^2$, where $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm (a.k.a. squared $L^2$ norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L= U Λ U^\\top$ when we replace $L = S S^\\top$ ans compute the equation for one eigenvalue and one eigenvector, we obtain $\\lambda_k =u_k  S S^\\top u_k^\\top$    = the squared euclidien norm of $S^\\top u_k$\n",
    "\n",
    "$u_k$ can be written as the inner product of the vector $S^\\top u_k$ with itself, which is the squared euclidien norm of $S^\\top u_k$, this shows that $u_k \\geq 0$ and that the eigenvalues of L are all non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the quantity $\\| S^\\top x \\|_2^2$ tell us about $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $ x^\\top L x = \\frac{1}{2} \\sum_{i,j = 0}^{n_{nodes}}{w( i,j )(x_i - x_j)^2}$\n",
    "\n",
    "And since  $L = S S^\\top$  .We obtain  $ x^\\top S^\\top S x = \\frac{1}{2} \\sum_{i,j = 0}^{n_{nodes}}{w( i,j ) (x_i - x_j)^2 }$ \n",
    "\n",
    "So   $\\| S^\\top x \\|_2^2 = \\frac{1}{2} \\sum_{i,j = 0}^{n_{nodes}}{w( i,j ) (x_i - x_j)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the value of $u_0$, both for the combinatorial and normalized Laplacians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u_0$ is the algebraic connectivity for both the combinatorial and normalized laplacians it's equal to zero and its multiplicity gives connectedness of graph. The dimension of the nullspace of L (the eigenspace of 0) is equal to the number of connected components of the underlying graph of G.\n",
    "The eigenvector associated with the algebraic connectivity has been named the *Fiedler vector*. The Fiedler vector can be used to partition a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Look at the spectrum of the Laplacian by plotting the eigenvalues.\n",
    "Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Eigenvalues')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eigenvalues)\n",
    "plt.title('Eigenvalues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the Spectrum of the laplacien confirms the theory that all the eigenvalues are real and non-negative since L is real, symmetric and PSD. We can also see that the majority of eigenvalues are close to 1. Additionaly, we see that a certain number of eigenvalues are equal to 0, by counting them, we can count the number of connected components in the graph, and thus answer to the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components are there in your graph? Answer using the eigenvalues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 connected components.\n"
     ]
    }
   ],
   "source": [
    "number_connected_components=np.where(eigenvalues== 0)\n",
    "a = len(number_connected_components[0])\n",
    "print(f'There are {a} connected components.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an upper bound on the eigenvalues, i.e., what is the largest possible eigenvalue? Answer for both the combinatorial and normalized Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the normalised Laplacian the largest possible eigenvalue is 2 . \n",
    "\n",
    "For the combinatorial Laplacian the upperbound on the eigenvalue is  $2 \\cdot \\Delta G$, with $\\Delta G$ being the largest degree of the Laplacian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Laplacian eigenmaps\n",
    "\n",
    "*Laplacian eigenmaps* is a method to embed a graph $\\mathcal{G}$ in a $d$-dimensional Euclidean space.\n",
    "That is, it associates a vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$.\n",
    "The graph $\\mathcal{G}$ is thus embedded as $Z \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What do we use Laplacian eigenmaps for? (Or more generally, graph embeddings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian eigenmaps is a technique used in order to dimentionally reduce the data. High dimentional data can be hard to visualize/interpret. Therefore if we assume that the data lies on an embedded manifold within the higher-dimensional space and if the manifold is of 2 or 3 dimensions for example, we can use the eigenmaps decomposition in order to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Embed your graph in $d=2$ dimensions with Laplacian eigenmaps.\n",
    "Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "\n",
    "**Recompute** the eigenvectors you need with a partial eigendecomposition method for sparse matrices.\n",
    "When $k \\ll N$ eigenvectors are needed, partial eigendecompositions are much more efficient than complete eigendecompositions.\n",
    "A partial eigendecomposition scales as $\\Omega(k |\\mathcal{E}|$), while a complete eigendecomposition costs $\\mathcal{O}(N^3)$ operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen earlier that our graph has 343 connected components. However most of these connected components are nodes that are not connected to any other node. We have only 2 components that have more than 1 node. And from those two, one of them nearly englobes all the nodes (9276 out of a total of 9628). Therefore this giant component will be the only one we will be considering for the Laplacian Eigenmaps embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will build the normalized laplacian of the giant component. \n",
    "# We will also build the combinatorial laplacian for comparasion.\n",
    "\n",
    "import src.find_components as fcomp\n",
    "# We need to have a connected graph.\n",
    "# As our network is not connected, we use the adjacency matrix of the largest connected component instead\n",
    "\n",
    "# The first element of the connectedIndices matrix is a vector of length equal to n_nodes\n",
    "# The value of its elements is 1 if the element is in the largest connected componenent and 0 if it is not\n",
    "connectedIndices = fcomp.find_components(adjacency)\n",
    "biggestComponentSize = np.amax(np.sum(connectedIndices, axis=1))\n",
    "\n",
    "# get the adjacency matrix of the largest connected component of our network\n",
    "indx=np.array(range(n_nodes),dtype=int)\n",
    "indx=connectedIndices[0].astype(int)*indx\n",
    "indx_without_zeros=np.unique(indx)\n",
    "Biggest_component=adjacency[indx_without_zeros,:][:,indx_without_zeros]\n",
    "\n",
    "degree = np.diag(np.sum(Biggest_component, axis=0))\n",
    "invroot_degree = np.diag(np.power(np.sum(Biggest_component, axis=0),-0.5))\n",
    "\n",
    "laplacian_biggest_component = degree - Biggest_component #combinatorial laplacian\n",
    "\n",
    "# Matrix mupltiplication as A*B seems to lose precision and rounds very small numbers to zero, giving a false normalized\n",
    "# Laplacian. Therefore we use np.matmul and store the intermidiate result in vairable A:\n",
    "\n",
    "A = np.matmul(invroot_degree,laplacian_biggest_component)\n",
    "laplacian_normalized_biggest_component = np.matmul(A,invroot_degree) #normalized laplacian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an unknown reason, sparse.linalg.eigs takes more time than the np.linalg.eig, therefore we will use the latter and take\n",
    "# the first two vectors associated to the first 2 non-zero eigenvalues => the 2nd and the 3rd eigenvectors in vecs.Note th\n",
    "# Note that eigh already sorts the eigenvalues (and the associaated eigenvectors) from smallest to largest.\n",
    "vals, vecs = np.linalg.eigh(laplacian_biggest_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same as above for the normalized laplacien\n",
    "valsn, vecsn = np.linalg.eigh(laplacian_normalized_biggest_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the nodes embedded in 2D. Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized laplacian 2d eigenmaps\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(vecsn[:,1],vecsn[:,2],s=5,alpha=0.6)\n",
    "plt.title('Not normalized by the degrees')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.divide(vecsn[:,1],np.diag(degree)),np.divide(vecsn[:,2],np.diag(degree)),s=5,alpha=0.6)\n",
    "plt.title('Normalized by the degrees') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe that we have a big cluster of points near 0. It's quite difficult to see what happens in that region \n",
    "# so let's zoom-in:\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(vecsn[:,1],vecsn[:,2],s=5,alpha=0.6)\n",
    "plt.title('Zoom, Not normalized by the degrees')\n",
    "plt.axis([-0.0002, 0.0008, -0.0008, 0.0008])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.divide(vecsn[:,1],np.diag(degree)),np.divide(vecsn[:,2],np.diag(degree)),s=5,alpha=0.6)\n",
    "plt.title('Zoom, Normalized by the degrees') \n",
    "plt.axis([-0.0001, 0.0001, -0.0001, 0.0001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the scale of the normalized graph is reduced and all of our points are very close together near zero. As the laplacian eigenmaps preserve the locality, this means that our nodes are also very close together in the high dimention state state, suggesting a very connected graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, we wanted to see what the combinatorial laplacian eigenmaps would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinatorial laplacian 2d eigenmaps\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.scatter(vecs[:,1],vecs[:,2],s=5,alpha=0.6)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title('Not normalized by the degrees')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(np.divide(vecs[:,1],np.diag(degree)),np.divide(vecs[:,2],np.diag(degree)),s=5,alpha=0.6)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title('Normalized by the degrees') \n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(vecs[:,1],vecs[:,2],s=5,alpha=0.6)\n",
    "plt.title('Zoom, Not normalized by the degrees')\n",
    "plt.axis([-0.0002, 0.0008, -0.0008, 0.0008])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(np.divide(vecs[:,1],np.diag(degree)),np.divide(vecs[:,2],np.diag(degree)),s=5,alpha=0.6)\n",
    "plt.title('Zoom, Normalized by the degrees') \n",
    "plt.axis([-0.0001, 0.0001, -0.0001, 0.0001])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplacian Eigenmaps embedding, preserves the locality of the data, i.e. the relative local distances of the data points. This means that data points that are close to one another in the high dementional (N) space, would also be close to eachother in the low dimentional (d) embedded space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spectral clustering\n",
    "\n",
    "*Spectral clustering* is a method to partition a graph into distinct clusters.\n",
    "The method associates a feature vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$, then runs [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) in the embedding space $\\mathbb{R}^d$ to assign each node $v_i \\in \\mathcal{V}$ to a cluster $c_j \\in \\mathcal{C}$, where $k = |\\mathcal{C}|$ is the number of desired clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Choose $k$ and $d$. How did you get to those numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to cluster the nodes (the actors) by using their ratings in each genre of film to which they participated. To do so, we first computed the mean of the ratings for each genre and for each actor. For example, the table could you look like the following :\n",
    "\n",
    "| Actor | Action genre | Science fiction genre | Fantasy genre | ... |\n",
    "|---|---|---|---|---|\n",
    "| Arnold Schwarzenegger | 6.7 | 8.2 | 0 | ... |\n",
    "| Ryan Reynold | 4 | 0 | 7.5 | ... |\n",
    "\n",
    "\n",
    "It would enable us to see if actors playing in the same kind of film are more interconnected.\n",
    "\n",
    "This dataset contains 18 different movie genres, thus $k = 18$. As we have the average rating of each genre for each node, our dataset will be clustered in a space of the same number of dimensions, so $d = k = 18$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "1. Embed your graph in $\\mathbb{R}^d$ as $Z \\in \\mathbb{R}^{N \\times d}$.\n",
    "   Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "1. If you want $k=2$ clusters, partition with the Fiedler vector. For $k > 2$ clusters, run $k$-means on $Z$. Don't implement $k$-means, use the `KMeans` class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 types of clustering\n",
    "We clustered using the 18 first eigenvectors, but also using the 18 feature vectors (the average rating for each movie type). We did this to observe if a method was better than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "k_clusters = 18\n",
    "\n",
    "# K-Means using the features associated to each actor\n",
    "\n",
    "people = pd.read_csv('data/features_v3.csv')\n",
    "\n",
    "people.fillna(0)\n",
    "\n",
    "genre_ratings = people.iloc[indx_without_zeros,5:23]\n",
    "\n",
    "trueLabels = people.iloc[indx_without_zeros,4]\n",
    "\n",
    "genre_ratings.fillna(0, inplace=True)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_clusters, random_state=0).fit(genre_ratings.values)\n",
    "\n",
    "predictedLabels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means based on the eigenvactors of the laplacian\n",
    "\n",
    "# We take the first 18 eigenvectors\n",
    "selected_eigenvectors=vecs[0:k_clusters].T\n",
    "selected_eigenvectors.shape #this is R 9276x18\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=k_clusters, random_state=0).fit(selected_eigenvectors)\n",
    "predictedLabels2 = kmeans2.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Use the computed cluster assignment to reorder the adjacency matrix $A$.\n",
    "What do you expect? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderMatrix(adjacency, predictedLabels):\n",
    "\n",
    "    s = adjacency.shape[0]\n",
    "\n",
    "    tmp_adjacency = np.zeros((s,s))\n",
    "    ordered_adjacency = np.zeros((s,s))\n",
    "    \n",
    "    k_clusters = np.unique(predictedLabels).shape[0]\n",
    "\n",
    "    a = 0\n",
    "    for k in range(0,k_clusters):\n",
    "        indexes = np.argwhere(predictedLabels==k)\n",
    "        for i in np.nditer(indexes):\n",
    "            tmp_adjacency[a,:] = adjacency[i,:]\n",
    "            a = a+1\n",
    "        \n",
    "    a = 0\n",
    "    for k in range(0,k_clusters):\n",
    "        indexes = np.argwhere(predictedLabels==k)\n",
    "        for i in np.nditer(indexes):\n",
    "            ordered_adjacency[:,a] = tmp_adjacency[:,i]\n",
    "            a = a+1\n",
    "    return ordered_adjacency;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplot(311)\n",
    "plt.spy(reorderMatrix(Biggest_component, predictedLabels), markersize=0.1)\n",
    "plt.title('Reordered adjacency matrix prior to ratings')\n",
    "plt.subplot(312)\n",
    "plt.spy(reorderMatrix(Biggest_component, predictedLabels2), markersize=0.1)\n",
    "plt.title('Reordered adjacency matrix prior to eigenvectors')\n",
    "plt.subplot(313)\n",
    "plt.spy(Biggest_component, markersize=0.1)\n",
    "plt.title('Original Adjacency matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were expecting to see a pattern in the adjacency matrix. \n",
    "\n",
    "This is indeed what we can observe on the reordered matrix prior to ratings. We can see some squares around the diagonal of the adjacency matrix, which means that actors in the same cluster are very connected, and have less conexion with other clusters. \n",
    "We can also see that one cluster has connection with almost everyother clusters (this can be seen with the vertical and horizontal bars around node 8000). This means that this cluster is composed of nodes that are very connected (hubs).\n",
    "\n",
    "For the rearranged matrix in prior to eigenvectors, we see almost no changes. This is because k-means clustered almost every node into the first cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "If you have ground truth clusters for your dataset, compare the cluster assignment from spectral clustering to the ground truth.\n",
    "A simple quantitative measure is to compute the percentage of nodes that have been correctly categorized.\n",
    "If you don't have a ground truth, qualitatively assess the quality of the clustering.\n",
    "\n",
    "Ground truth clusters are the \"real clusters\".\n",
    "For example, the genre of musical tracks in FMA, the category of Wikipedia articles, the spammer status of individuals, etc.\n",
    "Look for the `labels` in the [dataset descriptions](https://github.com/mdeff/ntds_2018/tree/master/projects/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ground Truth labels in our dataset\n",
    "As written in the dataset description, the labels should be the movie genre. The problem is that we do not have ground truth clusters for actors. The dataset contains a genre label for each movie, however as the actors played in different movies, they technically have multiple labels.\n",
    "\n",
    "So we decided that the genre label of each actor would be determined by the movie genre in which the actor acted the most.\n",
    "#### *Example*\n",
    "Arnold Schwarzenegger played in 76 action movies, 35 science fiction movies and 26 other genre of movies. Therefore, his label will be **action genre**.\n",
    "\n",
    "#### Assigning the correct genre to the new clusters\n",
    "Once we cluster with k-means, we obtain k clusters, but do not know what genre they represent. To do that, we count the number of actors in each cluster, and see what true labels are the most present. We then assign this label to the cluster.\n",
    "\n",
    "#### *Example*\n",
    "The first cluster contains 20 actors considered as action actors, 50 considered as Horror actors and 30 actors of other genres. Therefore the label assigned to this cluster will be **Horror**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFractionCorrectlyClassified(trueLabels, predictedLabels):\n",
    "    labels = pd.DataFrame({'true_labels' : trueLabels, 'idx_true_labels': 0, 'predicted_labels' : predictedLabels})\n",
    "\n",
    "    maxLabel = int(np.max(labels['true_labels']))\n",
    "\n",
    "    for i in range(0,maxLabel):\n",
    "        A = labels.query(f'true_labels == {i}')\n",
    "        B = A['predicted_labels'].value_counts()\n",
    "    \n",
    "        if B.empty == False:\n",
    "            predictedLabelOfGenre = B.idxmax()\n",
    "            for node in labels.iterrows():\n",
    "                if node[1]['true_labels'] == i:\n",
    "                    labels.loc[node[0],'idx_true_labels'] = predictedLabelOfGenre\n",
    "                \n",
    "    correctlyClassified = labels.query('idx_true_labels == predicted_labels').shape\n",
    "\n",
    "    fractionCorrectlyClassified = 100*correctlyClassified[0]/labels.shape[0]\n",
    "    \n",
    "    return fractionCorrectlyClassified;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcorrFeat = computeFractionCorrectlyClassified(trueLabels, predictedLabels)\n",
    "fcorrEig =computeFractionCorrectlyClassified(trueLabels, predictedLabels2)\n",
    "\n",
    "print(f'The number of well classified nodes by using the features (ratings) is {fcorrFeat}% and the number of well classified nodes by using the eigenvectors is {fcorrEig}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The clustering seems to assign a genre to each actor that does not correspond to the kind of film in which he played the most.\n",
    "\n",
    "If we cluster using the 18 first eigenvectors, we obtain 1 very large cluster that contains almost every nodes, and 17 smaller clusters. This might be because our initial graph is very connected, and that actors of different movie genres are still connected between each other. Thus k-means is not able to cluster actors with respect to their labeled genre.\n",
    "\n",
    "On the other hand, if we cluster using the features associated to each actors (their ratings by genre), we get 18 cluster of very similar size. Once again, this doesn't represent correctly the truth labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Plot the cluster assignment (one color per cluster) on the 2D embedding you computed above with Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "\n",
    "plt.figure(figsize=(10, 30))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.scatter(vecs[:,1],vecs[:,2],s=5, c=predictedLabels, cmap=viridis)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Cluster assignment by the features')\n",
    "\n",
    "dataPoints = vecs\n",
    "\n",
    "dataPoints[dataPoints>0.01]=0\n",
    "dataPoints[dataPoints<-0.01]=0\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.scatter(dataPoints[:,1],dataPoints[:,2],s=1, c=predictedLabels, cmap=viridis)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Cluster assignment zoom, features')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.scatter(dataPoints[:,1],dataPoints[:,2],s=1, c=predictedLabels2, cmap=viridis)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Cluster assignment zoom, eigenvectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Why did we use the eigenvectors of the graph Laplacian as features? Could we use other features for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the eigenvectors are orthonormal and embed information about the graph structure (how connected the graph is for example). Using them as features allow us to cluster in a lower-dimensional space that keeps the locality of the data in the original space.\n",
    "\n",
    "We tried using other features (the ratings of the actor for each genre), but it didn't give satisfying results. Of course, we could try using another set of features, but we think that the main issue is that the dataset doesn't have actual ground truth labels for the actors. The way we assigned ground truth labels to actors might not be the most optimal. Especially since most actors played in large variety of movie genres, thus assigning them 1 genre only might be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
