{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS'18 milestone 1: network collection and properties\n",
    "[Effrosyni Simou](https://lts4.epfl.ch/simou), [EPFL LTS4](https://lts4.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `4`\n",
    "* Students: `Julien Berger, JÃ©rÃ©my Jayet, Hana Samet, Mathieu Shiva`\n",
    "* Dataset: `IMDb Films and Crew `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to three sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this milestone is to start getting acquainted to the network that you will use for this class. In the first part of the milestone you will import your data using [Pandas](http://pandas.pydata.org) and you will create the adjacency matrix using [Numpy](http://www.numpy.org). This part is project specific. In the second part you will have to compute some basic properties of your network. **For the computation of the properties you are only allowed to use the packages that have been imported in the cell below.** You are not allowed to use any graph-specific toolboxes for this milestone (such as networkx and PyGSP). Furthermore, the aim is not to blindly compute the network properties, but to also start to think about what kind of network you will be working with this semester. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import queue as Q # Package used to manage queues\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Import your data and manipulate them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A. Load your data in a Panda dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should define and understand what are your nodes, what features you have and what are your labels. Please provide below a Panda dataframe where each row corresponds to a node with its features and labels. For example, in the the case of the Free Music Archive (FMA) Project, each row of the dataframe would be of the following form:\n",
    "\n",
    "\n",
    "| Track   |  Feature 1  | Feature 2 | . . . | Feature 518|  Label 1 |  Label 2 |. . .|Label 16|\n",
    "|:-------:|:-----------:|:---------:|:-----:|:----------:|:--------:|:--------:|:---:|:------:|\n",
    "|         |             |           |       |            |          |          |     |        |\n",
    "\n",
    "It is possible that in some of the projects either the features or the labels are not available. This is OK, in that case just make sure that you create a dataframe where each of the rows corresponds to a node and its associated features or labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the flag is set to 0 the feature dataframe is created from the initial dataset\n",
    "#When the flag is set to 1, the feature dataframe is loaded from a file\n",
    "#This avoids to compute the feature dataframe each time that we reload the kernel\n",
    "flag_features=0\n",
    "\n",
    "\n",
    "#Create the features from the original dataset\n",
    "if flag_features==0:\n",
    "    # The flattening of the data was achieved by using the code provided alongsided the dataset on kaggle \n",
    "    # The following section is simply the application of the following method :\n",
    "    #  https://www.kaggle.com/sohier/tmdb-format-introduction/notebook\n",
    "    #########################################################################################################################\n",
    "    #########################################################################################################################\n",
    "    movies = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "    movies['release_date'] = pd.to_datetime(movies['release_date']).apply(lambda x: x.date())\n",
    "    json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "    for column in json_columns:\n",
    "        movies[column] = movies[column].apply(json.loads)\n",
    "\n",
    "    credits = pd.read_csv('./data/tmdb_5000_credits.csv')\n",
    "    json_columns = ['cast', 'crew']\n",
    "    for column in json_columns:\n",
    "           credits[column] = credits[column].apply(json.loads)\n",
    "\n",
    "    #give a new movie index to replace the current movie index\n",
    "    new_movie_index=np.arange(movies.shape[0])  \n",
    "    movies['id']=new_movie_index\n",
    "    credits['movie_id']=new_movie_index  \n",
    "\n",
    "    def safe_access(container, index_values):\n",
    "        # return a missing value rather than an error upon indexing/key failure\n",
    "        result = container\n",
    "        try:\n",
    "            for idx in index_values:\n",
    "                result = result[idx]\n",
    "            return result\n",
    "        except IndexError or KeyError:\n",
    "            return pd.np.nan\n",
    "\n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['cast']], axis=1);\n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['crew']], axis=1);\n",
    "    credits.apply(lambda row: [person.update({'order': order}) for order, person in enumerate(row['crew'])], axis=1);\n",
    "\n",
    "    cast = []\n",
    "    credits.cast.apply(lambda x: cast.extend(x))\n",
    "    cast = pd.DataFrame(cast)\n",
    "    cast['type'] = 'cast'\n",
    "\n",
    "    crew = []\n",
    "    credits.crew.apply(lambda x: crew.extend(x))\n",
    "    crew = pd.DataFrame(crew)\n",
    "    crew['type'] = 'crew'\n",
    "\n",
    "    people = pd.concat([cast, crew],  ignore_index=True, sort=False)\n",
    "    #########################################################################################################################\n",
    "    #########################################################################################################################\n",
    "\n",
    "    #The rest of the section is our own code.\n",
    "\n",
    "    #Removing the useless columns and sorting the data by the unique ID of each person\n",
    "    people = people.drop(columns=['gender','department', 'credit_id','cast_id', 'job','order','character','type'])\n",
    "    people = people.sort_values(by='id')\n",
    "\n",
    "    #removing the rows with similar person and ID (For example if someone played 2 different roles in a movie we only keep one of these entries)\n",
    "    people=people.drop_duplicates(subset=['id', 'movie_id'])\n",
    "\n",
    "    #get the number of movie that each person worked on\n",
    "    table_nb_movies=people['id'].value_counts()\n",
    "    unique_values=people['id'].unique()\n",
    "\n",
    "    movies['movie_id']=movies['id']\n",
    "    movies=movies.drop(columns=['vote_count','genres','homepage','keywords','original_language','overview','popularity','production_companies','production_countries','runtime','spoken_languages','status','tagline','original_title'])\n",
    "    movies = movies.set_index('movie_id')\n",
    "    \n",
    "    \n",
    "    #Select the decade that you want to keep\n",
    "    startdate = pd.to_datetime(\"1980-01-01\").date()\n",
    "    enddate = pd.to_datetime(\"1990-01-01\").date()\n",
    "    movies=movies[(movies['release_date'] > startdate) & (movies['release_date'] <enddate)]\n",
    "    \n",
    "    #merge the movies and the people so that we can get the rating of each movie \n",
    "    people = people.merge(movies, on='movie_id')\n",
    "    people['id']=people['id_x']\n",
    "    people = people.drop(columns=['id_x','id_y'])\n",
    "    \n",
    "    #simple_list will contain all the different actor names, this will be the column of our features\n",
    "    simple_list=people.loc[:, ['id','movie_id','name']]\n",
    "    simple_list=simple_list.sort_values(by='id')\n",
    "    simple_list=simple_list.drop_duplicates('id')\n",
    "    simple_list=simple_list.set_index('id') \n",
    "    simple_list=simple_list.drop(columns=['movie_id'])\n",
    "    \n",
    "    #This code takes 14 minutes to run and removes all the people that worked on less than 5 movies\n",
    "    #This subsampling of the dataset is done to go from 104'000 nodes to only 9628\n",
    "    threshold_movies=5\n",
    "    for idx in unique_values:\n",
    "        nb_films=table_nb_movies[idx] \n",
    "        if (nb_films)<threshold_movies:\n",
    "            simple_list=simple_list.drop(index=idx)\n",
    "\n",
    "    #Calculate the number of people that did more than 5 movies\n",
    "    unique_id=simple_list['id'].unique()\n",
    "    unique_id.sort()\n",
    "\n",
    "    #Add a column that will contain the average rating of the actor \n",
    "    simple_list['Average_Rating']=np.nan\n",
    "\n",
    "    #Add one colum for all the existing movies\n",
    "    for i in new_movie_index:\n",
    "        simple_list['Movie_ID_%d' % i]=0\n",
    "\n",
    "    unique_id=simple_list['id'].unique()\n",
    "    unique_id.sort()\n",
    "    index_ini=0\n",
    "\n",
    "    #This code adds a 1 in the corresponding movie column of simple_list if the person worked in this movie  \n",
    "    #It also calculates the average rating of all the movies the person worked on and adds it in the AVERAGE_RATING column\n",
    "    for idx in unique_id:\n",
    "        rating_average=0\n",
    "        subset=people[people['id'] == idx]\n",
    "        new_index_subset = pd.Series(range(0,len(subset)))\n",
    "        subset=subset.set_index(new_index_subset) \n",
    "        index_person=subset.iloc[0,4]\n",
    "\n",
    "        for i in new_index_subset:\n",
    "            index_film=subset.iloc[i-1,0]+3\n",
    "            #simple_list.loc[idx, 'Movie_ID_%d' % index_film]=1\n",
    "            simple_list.iloc[index_ini, index_film]=1\n",
    "            rating_average=rating_average+subset.iloc[i-1,3]\n",
    "        rating_average=rating_average/len(subset)\n",
    "            #simple_list.loc[idx, 'Average_Rating']=rating_average\n",
    "        simple_list.iloc[index_ini, 2]=rating_average\n",
    "        index_ini=index_ini+1\n",
    "\n",
    "    #This simple_list is now our feature dataframe that contains a row for each node (the people),\n",
    "    # a column for each movie of the dataset that\n",
    "    features=simple_list\n",
    "    \n",
    "#load the features from a file\n",
    "if flag_features==1:\n",
    "    features = pd.read_csv('./data/features.csv') \n",
    "    features = features.drop(features.columns[0],axis=1)\n",
    "    features = features.drop(columns=['name'],axis=1)\n",
    "    \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#When the flag is set to 0 the feature dataframe is created from the initial dataset\n",
    "#When the flag is set to 1, the feature dataframe is loaded from a file\n",
    "#This avoids to compute the feature dataframe each time that we reload the kernel\n",
    "flag_features=1\n",
    "\n",
    "\n",
    "#Create the features from the original dataset\n",
    "if flag_features==0:\n",
    "    # The flattening of the data was achieved by using the code provided alongsided the dataset on kaggle \n",
    "    # The following section is simply the application of the following method :\n",
    "    #  https://www.kaggle.com/sohier/tmdb-format-introduction/notebook\n",
    "    #########################################################################################################################\n",
    "    #########################################################################################################################\n",
    "    movies = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "    movies['release_date'] = pd.to_datetime(movies['release_date']).apply(lambda x: x.date())\n",
    "    json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "    for column in json_columns:\n",
    "        movies[column] = movies[column].apply(json.loads)\n",
    "\n",
    "    credits = pd.read_csv('./data/tmdb_5000_credits.csv')\n",
    "    json_columns = ['cast', 'crew']\n",
    "    for column in json_columns:\n",
    "           credits[column] = credits[column].apply(json.loads)\n",
    "\n",
    "    #give a new movie index to replace the current movie index\n",
    "    new_movie_index=np.arange(movies.shape[0])  \n",
    "    movies['id']=new_movie_index\n",
    "    credits['movie_id']=new_movie_index  \n",
    "\n",
    "    def safe_access(container, index_values):\n",
    "        # return a missing value rather than an error upon indexing/key failure\n",
    "        result = container\n",
    "        try:\n",
    "            for idx in index_values:\n",
    "                result = result[idx]\n",
    "            return result\n",
    "        except IndexError or KeyError:\n",
    "            return pd.np.nan\n",
    "\n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['cast']], axis=1);\n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['crew']], axis=1);\n",
    "    credits.apply(lambda row: [person.update({'order': order}) for order, person in enumerate(row['crew'])], axis=1);\n",
    "\n",
    "    cast = []\n",
    "    credits.cast.apply(lambda x: cast.extend(x))\n",
    "    cast = pd.DataFrame(cast)\n",
    "    cast['type'] = 'cast'\n",
    "\n",
    "    crew = []\n",
    "    credits.crew.apply(lambda x: crew.extend(x))\n",
    "    crew = pd.DataFrame(crew)\n",
    "    crew['type'] = 'crew'\n",
    "\n",
    "    people = pd.concat([cast, crew],  ignore_index=True, sort=False)\n",
    "    #########################################################################################################################\n",
    "    #########################################################################################################################\n",
    "\n",
    "    #The rest of the section is our own code.\n",
    "\n",
    "    #Removing the useless columns and sorting the data by the unique ID of each person\n",
    "    people = people.drop(columns=['gender','department', 'credit_id','cast_id', 'job','order','character','type'])\n",
    "    people = people.sort_values(by='id')\n",
    "\n",
    "    #removing the rows with similar person and ID (For example if someone played 2 different roles in a movie we only keep one of these entries)\n",
    "    people=people.drop_duplicates(subset=['id', 'movie_id'])\n",
    "\n",
    "    #get the number of movie that each person worked on\n",
    "    table_nb_movies=people['id'].value_counts()\n",
    "    unique_values=people['id'].unique()\n",
    "\n",
    "    movies['movie_id']=movies['id']\n",
    "    movies=movies.drop(columns=['vote_count','budget','genres','homepage','keywords','original_language','overview','popularity','production_companies','production_countries','release_date','revenue','runtime','spoken_languages','status','tagline','original_title'])\n",
    "    movies = movies.set_index('movie_id')\n",
    "\n",
    "    #merge the movies and the people so that we can get the rating of each movie \n",
    "    people = people.merge(movies, on='movie_id')\n",
    "    people['id']=people['id_x']\n",
    "    people = people.drop(columns=['id_x','id_y'])\n",
    "\n",
    "    #simple_list will contain all the different actor names, this will be the column of our features\n",
    "    simple_list=people.loc[:, ['id','movie_id','name']]\n",
    "    simple_list=simple_list.sort_values(by='id')\n",
    "    simple_list=simple_list.drop_duplicates('id')\n",
    "    simple_list=simple_list.set_index('id') \n",
    "    simple_list=simple_list.drop(columns=['movie_id'])\n",
    "\n",
    "    #This code takes 14 minutes to run and removes all the people that worked on less than 5 movies\n",
    "    #This subsampling of the dataset is done to go from 104'000 nodes to only 9628\n",
    "    threshold_movies=5\n",
    "    for idx in unique_values:\n",
    "        nb_films=table_nb_movies[idx] \n",
    "        if (nb_films)<threshold_movies:\n",
    "            simple_list=simple_list.drop(index=idx)\n",
    "\n",
    "    #Calculate the number of people that did more than 5 movies\n",
    "    unique_id=simple_list['id'].unique()\n",
    "    unique_id.sort()\n",
    "\n",
    "    #Add a column that will contain the average rating of the actor \n",
    "    simple_list['Average_Rating']=np.nan\n",
    "\n",
    "    #Add one colum for all the existing movies\n",
    "    for i in new_movie_index:\n",
    "        simple_list['Movie_ID_%d' % i]=0\n",
    "\n",
    "    unique_id=simple_list['id'].unique()\n",
    "    unique_id.sort()\n",
    "    index_ini=0\n",
    "\n",
    "    #This code adds a 1 in the corresponding movie column of simple_list if the person worked in this movie  \n",
    "    #It also calculates the average rating of all the movies the person worked on and adds it in the AVERAGE_RATING column\n",
    "    for idx in unique_id:\n",
    "        rating_average=0\n",
    "        subset=people[people['id'] == idx]\n",
    "        new_index_subset = pd.Series(range(0,len(subset)))\n",
    "        subset=subset.set_index(new_index_subset) \n",
    "        index_person=subset.iloc[0,4]\n",
    "\n",
    "        for i in new_index_subset:\n",
    "            index_film=subset.iloc[i-1,0]+3\n",
    "            #simple_list.loc[idx, 'Movie_ID_%d' % index_film]=1\n",
    "            simple_list.iloc[index_ini, index_film]=1\n",
    "            rating_average=rating_average+subset.iloc[i-1,3]\n",
    "        rating_average=rating_average/len(subset)\n",
    "            #simple_list.loc[idx, 'Average_Rating']=rating_average\n",
    "        simple_list.iloc[index_ini, 2]=rating_average\n",
    "        index_ini=index_ini+1\n",
    "\n",
    "    #This simple_list is now our feature dataframe that contains a row for each node (the people),\n",
    "    # a column for each movie of the dataset that\n",
    "    features=simple_list\n",
    "    \n",
    "#load the features from a file\n",
    "if flag_features==1:\n",
    "    features = pd.read_csv('./data/features.csv') \n",
    "    features = features.drop(features.columns[0],axis=1)\n",
    "    features = features.drop(columns=['name'],axis=1)\n",
    "    \n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code used to generate milestone3 featurematrix AND DATE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "movies['release_date'] = pd.to_datetime(movies['release_date']).apply(lambda x: x.date())\n",
    "json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "for column in json_columns:\n",
    "    movies[column] = movies[column].apply(json.loads)\n",
    "\n",
    "credits = pd.read_csv('./data/tmdb_5000_credits.csv')\n",
    "json_columns = ['cast', 'crew']\n",
    "for column in json_columns:\n",
    "       credits[column] = credits[column].apply(json.loads)\n",
    "\n",
    "#give a new movie index to replace the current movie index\n",
    "new_movie_index=np.arange(movies.shape[0])  \n",
    "movies['id']=new_movie_index\n",
    "credits['movie_id']=new_movie_index  \n",
    "\n",
    "def safe_access(container, index_values):\n",
    "    # return a missing value rather than an error upon indexing/key failure\n",
    "    result = container\n",
    "    try:\n",
    "        for idx in index_values:\n",
    "            result = result[idx]\n",
    "        return result\n",
    "    except IndexError or KeyError:\n",
    "        return pd.np.nan\n",
    "    \n",
    "credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['cast']], axis=1);\n",
    "credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['crew']], axis=1);\n",
    "credits.apply(lambda row: [person.update({'order': order}) for order, person in enumerate(row['crew'])], axis=1);\n",
    "\n",
    "movies.apply(lambda row: [x.update({'id': row['id']}) for x in row['genres']], axis=1);\n",
    "#credits.apply(lambda row: [person.update({'order': order}) for order, person in enumerate(row['crew'])], axis=1);\n",
    "\n",
    "#movies.loc[movies.genres ==[], 'genres'] = ['Unknown']\n",
    "\n",
    "genre = []\n",
    "movies.genres.apply(lambda x: genre.extend(x))\n",
    "genre = pd.DataFrame(genre)\n",
    "genre['type'] = 'genre'\n",
    "\n",
    "genre=genre.drop_duplicates('id')\n",
    "genre['index']=genre['id']\n",
    "genre=genre.set_index('id') \n",
    "list_of_genres=genre['name'].unique()\n",
    "#this list contains the genre types\n",
    "#There are 21 different genres\n",
    "nb_genres=len(list_of_genres)\n",
    "list_of_genres_id=pd.Series(range(nb_genres))\n",
    "list_of_genres_id\n",
    "\n",
    "cast = []\n",
    "credits.cast.apply(lambda x: cast.extend(x))\n",
    "cast = pd.DataFrame(cast)\n",
    "cast['type'] = 'cast'\n",
    "\n",
    "crew = []\n",
    "credits.crew.apply(lambda x: crew.extend(x))\n",
    "crew = pd.DataFrame(crew)\n",
    "crew['type'] = 'crew'\n",
    "\n",
    "people = pd.concat([cast, crew],  ignore_index=True, sort=False)\n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "#The rest of the section is our own code.\n",
    "\n",
    "#Removing the useless columns and sorting the data by the unique ID of each person\n",
    "people = people.drop(columns=['gender','department', 'credit_id','cast_id', 'job','order','character','type'])\n",
    "people = people.sort_values(by='id')\n",
    "\n",
    "#removing the rows with similar person and ID (For example if someone played 2 different roles in a movie we only keep one of these entries)\n",
    "people=people.drop_duplicates(subset=['id', 'movie_id'])\n",
    "\n",
    "#get the number of movie that each person worked on\n",
    "table_nb_movies=people['id'].value_counts()\n",
    "\n",
    "movies['movie_id']=movies['id']\n",
    "movies=movies.drop(columns=['vote_count','budget','genres','homepage','keywords','original_language','overview','popularity','production_companies','production_countries','revenue','runtime','spoken_languages','status','tagline','original_title'])\n",
    "movies=movies.set_index('movie_id')\n",
    "movies['genre']=genre['name']\n",
    "\n",
    "#DECADE SELECTION\n",
    "    #Select the decade that you want to keep\n",
    "startdate = pd.to_datetime(\"2010-01-01\").date()\n",
    "enddate = pd.to_datetime(\"2020-01-01\").date()\n",
    "movies=movies[(movies['release_date'] > startdate) & (movies['release_date'] <enddate)]\n",
    "\n",
    "#merge the movies and the people so that we can get the rating of each movie \n",
    "people = people.merge(movies, on='movie_id')\n",
    "unique_values=people['id_x'].unique()\n",
    "unique_values.sort()\n",
    "people['id']=people['id_x']\n",
    "people = people.drop(columns=['id_x','id_y','release_date'])\n",
    "\n",
    "#simple_list will contain all the different actor names, this will be the column of our features\n",
    "simple_list=people.loc[:, ['id','movie_id','name']]\n",
    "simple_list=simple_list.sort_values(by='id')\n",
    "simple_list=simple_list.drop_duplicates('id')\n",
    "\n",
    "simple_list=simple_list.set_index('id') \n",
    "simple_list=simple_list.drop(columns=['movie_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This code takes 14 minutes to run and removes all the people that worked on less than 5 movies\n",
    "##This subsampling of the dataset is done to go from 104'000 nodes to only 9628\n",
    "threshold_movies=5\n",
    "for idx in unique_values:\n",
    "    nb_films=table_nb_movies[idx] \n",
    "    if (nb_films)<threshold_movies:\n",
    "        simple_list=simple_list.drop(index=idx)\n",
    "simple_list.to_csv('./data/simple_10_20.csv', sep=','); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load simple list from file\n",
    "simple_list = pd.read_csv('./data/simple_10_20.csv') \n",
    "\n",
    "#Add a column that will contain the average rating of the actor \n",
    "simple_list['Average_Rating']=np.nan\n",
    "\n",
    "#Add a column for the ratings by type\n",
    "for i in range(len(list_of_genres)):\n",
    "    a=list_of_genres[i]\n",
    "    simple_list[a+'_Rating']=0\n",
    "#Add one colum for all the existing movies\n",
    "for i in new_movie_index:\n",
    "    simple_list['Movie_ID_%d' % i]=0\n",
    "    \n",
    "unique_id=simple_list['id'].unique()\n",
    "#unique_id=simple_list.index.unique()\n",
    "unique_id.sort()\n",
    "#unique_id.sort_values()\n",
    "index_ini=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code adds a 1 in the corresponding movie column of simple_list if the person worked in this movie  \n",
    "#It also calculates the average rating of all the movies the person worked on and adds it in the AVERAGE_RATING column\n",
    "\n",
    "index_ini=0\n",
    "for idx in unique_id:\n",
    "    rating_average=1\n",
    "    subset=people[people['id'] == idx]\n",
    "    new_index_subset = pd.Series(range(0,len(subset)))\n",
    "    subset=subset.set_index(new_index_subset) \n",
    "    index_person=subset.iloc[0,5]\n",
    "\n",
    "    rating_type=[0] * nb_genres\n",
    "    nb_movies_of_type=[0] * nb_genres\n",
    "    for i in new_index_subset:\n",
    "        index_film=subset.iloc[i,0]+23 #3 initial columns+21 new genre ratings\n",
    "        simple_list.iloc[index_ini, index_film]=1\n",
    "\n",
    "        #find the genre of the movie, and add this rating to the correct line\n",
    "        genre_of_the_movie=subset.loc[i,'genre']\n",
    "        for j in range(0,nb_genres):\n",
    "            if list_of_genres[j]==genre_of_the_movie:\n",
    "                rating_type[j]= rating_type[j]+subset.loc[i,'vote_average']\n",
    "                nb_movies_of_type[j]=nb_movies_of_type[j]+1\n",
    "                continue\n",
    "        #simple_list.iloc[index_ini, index_film]=simple_list.iloc[index_ini, index_film]+subset.iloc[i-1,3]\n",
    "        #simple_list.loc[idx, 'Average_Rating']=rating_average\n",
    "        rating_average=rating_average+subset.iloc[i,3]    \n",
    "    #nb_movies_of_type[nb_movies_of_type==0]=1\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    rating_type=np.divide(rating_type,nb_movies_of_type)\n",
    "\n",
    "    #placing the average rating per type\n",
    "    for j in range(0,nb_genres):\n",
    "        simple_list.iloc[index_ini, 3+j]=rating_type[j]\n",
    "\n",
    "    rating_average=rating_average/len(subset)\n",
    "    #simple_list.loc[idx, 'Average_Rating']=rating_average\n",
    "    simple_list.iloc[index_ini, 2]=rating_average    \n",
    "    index_ini=index_ini+1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_list.to_csv('./data/features_10_20.csv', sep=','); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('./data/features_60_80.csv')\n",
    "features = features.drop(features.columns[0],axis=1)\n",
    "features = features.drop(columns=['name'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the adjacency matrix of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there are edges connecting the attributed nodes that you organized in the dataframe above. The connectivity of the network is captured by the adjacency matrix $W$. If $N$ is the number of nodes, the adjacency matrix is an $N \\times N$ matrix where the value of $W(i,j)$ is the weight of the edge connecting node $i$ to node $j$.  \n",
    "\n",
    "There are two possible scenarios for your adjacency matrix construction, as you already learned in the tutorial by Benjamin:\n",
    "\n",
    "1) The edges are given to you explicitly. In this case you should simply load the file containing the edge information and parse it in order to create your adjacency matrix. See how to do that in the  [graph from edge list]() demo.\n",
    "\n",
    "2) The edges are not given to you. In that case you will have to create a feature graph. In order to do that you will have to chose a distance that will quantify how similar two nodes are based on the values in their corresponding feature vectors. In the [graph from features]() demo Benjamin showed you how to build feature graphs when using Euclidean distances between feature vectors. Be curious and explore other distances as well! For instance, in the case of high-dimensional feature vectors, you might want to consider using the cosine distance. Once you compute the distances between your nodes you will have a fully connected network. Do not forget to sparsify by keeping the most important edges in your network.\n",
    "\n",
    "Follow the appropriate steps for the construction of the adjacency matrix of your network and provide it in the Numpy array ``adjacency`` below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the flag is set to 0 the adjacency matrix dataframe is created from the feature matrix\n",
    "#When the flag is set to 1, the adjency matrix is loaded from a file\n",
    "#This avoids to compute the feature dataframe each time that we reload the kernel\n",
    "flag_adjacency=0\n",
    "\n",
    "#we save the columns of the feature dataframe that contain the id and the average rating so that we can\n",
    "#only keep a binary matrix that links people to the movies they worked on\n",
    "backup_id=features['id']\n",
    "backup_rating=features['Average_Rating']\n",
    "\n",
    "#Create the adjacency matrix from the feature dataframe\n",
    "if flag_adjacency==0:\n",
    "    \n",
    "    #We calculate the number of nodes   \n",
    "    n_nodes=len(features)\n",
    "    \n",
    "    #Create a matrix of 0 of size 9628x9628 (the number of nodes)\n",
    "    adjacency = np.zeros((n_nodes, n_nodes), dtype=int)\n",
    "    features = features.drop(columns=['id','Average_Rating'],axis=1)\n",
    "    \n",
    "    for idx_mul in range(n_nodes):\n",
    "        #for each different people, multiply his feature row to the entire matrix and stock this in a temporary array\n",
    "        tmp=features.multiply(features.iloc[idx_mul])\n",
    "        #On each row of the adjacency matrix, sum all the values of this temporary array\n",
    "        adjacency[idx_mul]=tmp.sum(axis=1)\n",
    "    \n",
    "    #fill the diagonal of the matrix with zeroes\n",
    "    np.fill_diagonal(adjacency, 0)\n",
    "    \n",
    "    np.save('./data/adjacency_60_80', adjacency);\n",
    "    \n",
    "#load the adjacency matrix from a file  \n",
    "if flag_adjacency==1:\n",
    "    adjacency = np.load('./data/adjacency.npy')\n",
    "    \n",
    "    #We calculate the number of nodes   \n",
    "    n_nodes=adjacency.shape[0]\n",
    "\n",
    "\n",
    "#We make the matrix more sparse by removing the link between two people if they worked only on 1 movie together\n",
    "adjacency[adjacency <2]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to plot the (weighted) adjacency matrix of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.spy(adjacency, markersize=0.1)\n",
    "plt.title('adjacency matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the maximum number of links $L_{max}$ in a network with $N$ nodes (where $N$ is the number of nodes in your network)? How many links $L$ are there in your collected network? Comment on the sparsity of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of links in our network:\n",
    "L = np.count_nonzero(adjacency)/2 \n",
    "\n",
    "# Maximum possible number of links in our network\n",
    "Lmax= n_nodes*(n_nodes-1)/2\n",
    "\n",
    "ratio= L/Lmax*100\n",
    "print('The maxmimum number of links Lmax is {:.0f}'.format(Lmax))\n",
    "print('The number of links L is {:.0f}'.format(L))\n",
    "print('The ratio between Lmax and L is {:.2f}%'.format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering a fully connected undirected network with N nodes, the maximum number of links (Lmax) would be N*(N-1)/2. Here we only have 0.28% of the possible links, meaning that our adjacency matrix is quite sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Is your graph directed or undirected? If it is directed, convert it to an undirected graph by symmetrizing the adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is *undirected*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code as the graph is undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "In the cell below save the features dataframe and the **symmetrized** adjacency matrix. You can use the Pandas ``to_csv`` to save the ``features`` and Numpy's ``save`` to save the ``adjacency``. We will reuse those in the following milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to save them each time\n",
    "\n",
    "#features.to_csv('./data/features_backup.csv', sep=','); \n",
    "#np.save('./data/adjacency_backup', adjacency);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are the edges of your graph weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the weight of and edge is equal to the number of movies that its two nodes have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the degree distibution of your network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_adjacency=adjacency\n",
    "unweighted_adjacency[unweighted_adjacency != 0] = 1;\n",
    "degree = np.sum(unweighted_adjacency, axis=0); # Your code here. It should be a numpy array.\n",
    "\n",
    "assert len(degree) == n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the histogram of the degree distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones_like(degree) / float(n_nodes)\n",
    "plt.hist(degree, weights=weights, rwidth=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.mean(degree)\n",
    "print('The average degree is {:.2f}'.format(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Comment on the degree distribution of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big portion of our nodes (around 85%) that have a rather small number of degrees. However we can also see that there is also a few nodes with a much bigger number of degrees that could probably suggest the existance of a giant component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Write a function that takes as input the adjacency matrix of a graph and determines whether the graph is connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_graph(adjacency):\n",
    "    \"\"\"Determines whether a graph is connected.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the graph is connected, False otherwise.\n",
    "    \"\"\"\n",
    "    # Init connected\n",
    "    connected = False\n",
    "\n",
    "    # Creates a node list with the number nodes contained in the adjacency matrix\n",
    "    nodeList = adjacency.shape\n",
    "    nodeList = nodeList[0]\n",
    "    nodeList = np.full(nodeList, np.nan)\n",
    "\n",
    "    # Initialize a queue\n",
    "    queueBuffer = Q.Queue()\n",
    "\n",
    "    # Initialize an array containing all the indexes of all the non-zero elements connecting the the 0 node with the others\n",
    "    connectedNodesTo0 = np.nonzero(adjacency[0,:])\n",
    "\n",
    "    # Init the queue\n",
    "    for i in np.nditer(connectedNodesTo0):\n",
    "        queueBuffer.put(i)\n",
    "\n",
    "    # Test each node if they are connected\n",
    "    #\n",
    "    # 1. Get the node in the queue and the distance from the source of the previous node.\n",
    "    # 2. If the node has not been assigned a distance yet, assign it the distance of the previous node + 1.\n",
    "    # 3. Get all the connected nodes and put them in the queue.\n",
    "    # 4. When there is no more nodes in the queue, exit the loop.\n",
    "    #\n",
    "    while queueBuffer.empty() == False :\n",
    "        node = queueBuffer.get()\n",
    "        tmp = np.nonzero(adjacency[node,:])\n",
    "        for j in np.nditer(tmp):\n",
    "            if np.isnan(nodeList[j]):\n",
    "                nodeList[j] = 1\n",
    "                queueBuffer.put(j)\n",
    "\n",
    "    nbOfConnectedNodes = np.nansum(nodeList)\n",
    "    nbOfExpectedConnectedNodes = nodeList.size\n",
    "\n",
    "    if nbOfConnectedNodes == nbOfExpectedConnectedNodes :\n",
    "        connected = True\n",
    "    else :\n",
    "        connected = False\n",
    "\n",
    "    return connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your graph connected? Run the ``connected_graph`` function to determine your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_graph(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Write a function that extracts the connected components of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_components(adjacency):\n",
    "    \"\"\"Find the connected components of a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of numpy arrays\n",
    "        A list of adjacency matrices, one per connected component.\n",
    "    \"\"\"\n",
    "    #initializing the array used to store the indices of nodes in each connected component \n",
    "    connectedIndices = np.zeros((adjacency.shape[0],adjacency.shape[0]))\n",
    "    \n",
    "    #array used to keep track of the visited nodes (NaN = not visited):\n",
    "    nodeList = np.full(adjacency.shape[0], np.nan)\n",
    "    \n",
    "    #initializing a queue to store connected nodes\n",
    "    queueBuffer = Q.Queue()\n",
    "    \n",
    "  \n",
    "    \n",
    "    for i in range(adjacency.shape[0]):\n",
    "        if np.isnan(nodeList[i]):\n",
    "            #if the node has no connections to the other nodes\n",
    "            if sum(adjacency[i,:]) == 0:\n",
    "                connectedIndices[i,i] = 1\n",
    "                nodeList[i] = 1\n",
    "            else:\n",
    "                #initializing the connections of node i\n",
    "                connectionsToI = np.nonzero(adjacency[i,:])\n",
    "                \n",
    "                #We will add to queue the nodes connected to the first not visited (=NaN) node in nodeList \n",
    "                for k in np.nditer(connectionsToI):\n",
    "                    queueBuffer.put(k)\n",
    "                \n",
    "                #extracting all the nodes that have a path to node i\n",
    "                while queueBuffer.empty() == False:\n",
    "                    node = queueBuffer.get()\n",
    "                    tmp = np.nonzero(adjacency[node,:])\n",
    "                    for j in np.nditer(tmp):\n",
    "                        if np.isnan(nodeList[j]):\n",
    "                            nodeList[j] = 1\n",
    "                            queueBuffer.put(j)\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        connectedIndices[i,:]=nodeList\n",
    "                    else:\n",
    "                        connectedIndices[i,:]=nodeList-np.nansum(connectedIndices, axis=0)\n",
    "   \n",
    "    #converting NaN values to zeros:\n",
    "    connectedIndices=np.nan_to_num(connectedIndices)\n",
    "    \n",
    "    #deleting zero lines:\n",
    "    connectedIndices = connectedIndices[~(connectedIndices==0).all(1)]\n",
    "    \n",
    "    #Now let's build a 3D matrix with the adjecency matrices of each connected componnent\n",
    "    \n",
    "    n_components = connectedIndices.shape[0] #number of total connected components, including isolated nodes\n",
    "    #components = np.zeros((n_components,adjacency.shape[0],adjacency.shape[0]))    \n",
    "\n",
    "    return connectedIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components is your network composed of? What is the size of the largest connected component? Run the ``find_components`` function to determine your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectedIndices = find_components(adjacency)\n",
    "\n",
    "biggestComponentSize = np.amax(np.sum(connectedIndices, axis=1))\n",
    "print('The biggest component contains {:.0f} nodes.'.format(biggestComponentSize))\n",
    "nbOfComponents = connectedIndices.shape[0]\n",
    "print('There are {:.0f} different connected components in the graph.'.format(nbOfComponents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Write a function that takes as input the adjacency matrix and a node (`source`) and returns the length of the shortest path between that node and all nodes in the graph using Dijkstra's algorithm. **For the purposes of this assignment we are interested in the hop distance between nodes, not in the sum of weights. **\n",
    "\n",
    "Hint: You might want to mask the adjacency matrix in the function ``compute_shortest_path_lengths`` in order to make sure you obtain a binary adjacency matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shortest_path_lengths(adjacency, source):\n",
    "    \"\"\"Compute the shortest path length between a source node and all nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    source: int\n",
    "        The source node. A number between 0 and n_nodes-1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of ints\n",
    "        The length of the shortest path from source to all nodes. Returned list\n",
    "        should be of length n_nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a node list with the number of nodes contained in the adjacency\n",
    "    # matrix\n",
    "    nodeList = adjacency.shape\n",
    "    nodeList = nodeList[0]\n",
    "    nodeList = np.full(nodeList, np.nan)\n",
    "\n",
    "    # Set the distance of the source node to 0\n",
    "    nodeList[source] = 0\n",
    "\n",
    "    # Initialize a queue\n",
    "    queueBuffer = Q.Queue()\n",
    "\n",
    "    # Initialize an array containing all the indexes of all the non-zero\n",
    "    # elements connecting the the 0 node with the others\n",
    "    connectedNodesToSource = np.nonzero(adjacency[source,:])\n",
    "\n",
    "    logging.debug(f'These nodes are connected to {source} : {connectedNodesToSource[0]}')\n",
    "\n",
    "    # Init the queue and set the distances of the adjacent nodes\n",
    "    # (to the source) to 1\n",
    "    if connectedNodesToSource[0].size != 0:\n",
    "        for i in np.nditer(connectedNodesToSource):\n",
    "            nodeList[i] = 1\n",
    "            queueBuffer.put(i)\n",
    "    else:\n",
    "        logging.warning(f'There is no node connected to {source}')\n",
    "        return nodeList\n",
    "\n",
    "    # Iterate over the nodes and calculate their distance\n",
    "    while queueBuffer.empty() == False :\n",
    "        node = queueBuffer.get() # Get the node in the queue\n",
    "        tmp = np.nonzero(adjacency[node,:]) # Get a list of the connected nodes\n",
    "        for j in np.nditer(tmp): # Iterate over the connected nodes\n",
    "            if np.isnan(nodeList[j]): # Eliminate the ones that are already been processed\n",
    "                nodeList[j] = nodeList[node]+1 # Add, for each node, their distance to the source node using the distance of the parent node\n",
    "                queueBuffer.put(j) # Place it in the queue\n",
    "\n",
    "    shortest_path_lengths = np.array(nodeList, np.int32)\n",
    "    return shortest_path_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "The diameter of the graph is the length of the longest shortest path between any pair of nodes. Use the above developed function to compute the diameter of the graph (or the diameter of the largest connected component of the graph if the graph is not connected). If your graph (or largest connected component) is very large, computing the diameter will take very long. In that case downsample your graph so that it has 1.000 nodes. There are many ways to reduce the size of a graph. For the purposes of this milestone you can chose to randomly select 1.000 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_diameter(adjacency,nbOfSamples=1000):\n",
    "\n",
    "    # Init diameter\n",
    "    diameter = 0\n",
    "\n",
    "    # Get the number of nodes\n",
    "    nbOfNodes = (adjacency.shape)[0]\n",
    "\n",
    "    # Init an nparray to contain all the length between nodes\n",
    "    shortest_path_lengths = np.zeros(nbOfNodes, np.int32)\n",
    "\n",
    "    # Make a sampling of the nodes but if the number of samples exceed the\n",
    "    # number of nodes, keep the list of nodes.\n",
    "    if nbOfSamples >= nbOfNodes:\n",
    "        samples = range(0,nbOfNodes)\n",
    "    else:\n",
    "        samples = np.random.randint(0,nbOfNodes,(nbOfSamples))\n",
    "\n",
    "    logging.info(f'nbOfNodes = {nbOfNodes}')\n",
    "    logging.info(f'nbOfSamples = {nbOfSamples}')\n",
    "\n",
    "    # Compute the distance for each node in the list between itself and all the\n",
    "    # other nodes\n",
    "    for i in samples:\n",
    "        logging.info(f'Testing node {i} and diameter = {diameter}')\n",
    "\n",
    "        shortest_path_lengths = compute_shortest_path_lengths(adjacency, i)\n",
    "        maxVal = np.amax(shortest_path_lengths)\n",
    "        if maxVal > diameter:\n",
    "            diameter = maxVal\n",
    "\n",
    "    return diameter\n",
    "\n",
    "diameter = compute_diameter(adjacency)\n",
    "print('The largest component of our graph has a diameter equal to {:.0f}'.format(diameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Write a function that takes as input the adjacency matrix, a path length, and two nodes (`source` and `target`), and returns the number of paths of the given length between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paths(adjacency, source, target, length):\n",
    "    \"\"\"Compute the number of paths of a given length between a source and target node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    source: int\n",
    "        The source node. A number between 0 and n_nodes-1.\n",
    "    target: int\n",
    "        The target node. A number between 0 and n_nodes-1.\n",
    "    length: int\n",
    "        The path length to be considered.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of paths.\n",
    "    \"\"\"\n",
    "\n",
    "    n_paths = 0\n",
    "    node = source\n",
    "    nodeDist = 0\n",
    "\n",
    "    # Creates a node list with the number nodes contained in the adjacency matrix\n",
    "    nbOfNodes = (adjacency.shape)[0]\n",
    "\n",
    "    # Initialize a queue\n",
    "    queueBuffer = Q.Queue()\n",
    "\n",
    "    # Init the queue\n",
    "    queueBuffer.put((source, 0))\n",
    "\n",
    "    # Test the case where source == target\n",
    "    if source == target :\n",
    "        n_paths = 0\n",
    "        return n_paths\n",
    "\n",
    "    # 1. Get the node in the queue and its distance from the source\n",
    "    # 2. Find the adjacent nodes\n",
    "    # 3. Test if the adjacent nodes are the target node and at a good distance\n",
    "    #   - If *yes*, continue to the next node in the list\n",
    "    #   - If *no*, but you are at a good distance, continue to the next node\n",
    "    #       in the list without incrementing the path counter\n",
    "    #   - Else, juste add the node in the queue for further process\n",
    "    while queueBuffer.empty() == False :\n",
    "        (node,nodeDist) = queueBuffer.get()\n",
    "\n",
    "        tmp = np.nonzero(adjacency[node,:])\n",
    "        for j in np.nditer(tmp):\n",
    "            if j == target and (nodeDist+1) == length :\n",
    "                n_paths = n_paths + 1\n",
    "                continue\n",
    "            elif (nodeDist+1) == length :\n",
    "                continue\n",
    "            else:\n",
    "                queueBuffer.put((j,nodeDist+1))\n",
    "\n",
    "    return n_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function on 5 pairs of nodes, with different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_paths(adjacency, 0, 10, 1))\n",
    "print(compute_paths(adjacency, 0, 10, 2))\n",
    "print(compute_paths(adjacency, 0, 10, 3))\n",
    "print(compute_paths(adjacency, 23, 67, 2))\n",
    "print(compute_paths(adjacency, 15, 93, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "How many paths of length 3 are there in your graph? Hint: calling the `compute_paths` function on every pair of node is not an efficient way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path=np.sum(np.linalg.matrix_power(adjacency,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "Write a function that takes as input the adjacency matrix of your graph (or of the largest connected component of your graph) and a node and returns the clustering coefficient of that node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_coefficient(adjacency, node):\n",
    "    \"\"\"Compute the clustering coefficient of a node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    node: int\n",
    "        The node whose clustering coefficient will be computed. A number between 0 and n_nodes-1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The clustering coefficient of the node. A number between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    adjacency[adjacency !=0]=1 #masking to get the unweighted matrix \n",
    "    clustering_coefficient=0\n",
    "    idx_neighbours=np.nonzero(adjacency[node])  #get the indices of the neighbours\n",
    "    number_neighbours=idx_neighbours[0].size    #get the number of neighbours\n",
    "    \n",
    "    if number_neighbours<2: #if only 0 or 1 neighbour, the clustering coefficient is equal to 0\n",
    "           return 0 \n",
    "    \n",
    "    #For all neighbours, see if they have links between them\n",
    "    for j in range(number_neighbours):\n",
    "        idx_test=idx_neighbours[0][j]\n",
    "        nb_similar_neighbours=adjacency[node]*adjacency[idx_test]\n",
    "        clustering_coefficient=clustering_coefficient+sum(nb_similar_neighbours)\n",
    "    \n",
    "    clustering_coefficient=clustering_coefficient/(number_neighbours*(number_neighbours-1))\n",
    "    #no need to multiply by 2 because we already count each link twice\n",
    "    \n",
    "    return clustering_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "What is the average clustering coefficient of your graph (or of the largest connected component of your graph if your graph is disconnected)? Use the function ``compute_clustering_coefficient`` to determine your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clustering_coeff=0\n",
    "\n",
    "#Use the previous function on each node of the graph and take the average\n",
    "for i in range (n_nodes):\n",
    "    tmp=compute_clustering_coefficient(adjacency, i)\n",
    "    average_clustering_coeff=average_clustering_coeff+tmp\n",
    "    \n",
    "average_clustering_coeff=average_clustering_coeff/n_nodes\n",
    "print('The average clustering coefficient is {:.4f}'.format(average_clustering_coeff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
